blocks {
  idx: 0
  parent_idx: -1
  vars {
    name: "__control_var@0.6105734521044512"
    type {
      type: LOD_TENSOR
    }
  }
  vars {
    name: "__control_var@0.8252923025577215"
    type {
      type: LOD_TENSOR
    }
  }
  vars {
    name: "y1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: INT64
          dims: -1
          dims: 1
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "__control_var@0.5934423733003124"
    type {
      type: LOD_TENSOR
    }
  }
  vars {
    name: "lstm_0.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 112
        }
      }
    }
    persistable: true
  }
  vars {
    name: "lstm_0.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 16
        }
        lod_level: 1
      }
    }
    persistable: false
  }
  vars {
    name: "fc_0.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
        }
      }
    }
    persistable: true
  }
  vars {
    name: "fc_0.tmp_0@GRAD"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 64
        }
        lod_level: 0
      }
    }
  }
  vars {
    name: "fc_0.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 64
        }
        lod_level: 1
      }
    }
    persistable: false
  }
  vars {
    name: "lstm_0.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 16
          dims: 64
        }
      }
    }
    persistable: true
  }
  vars {
    name: "sequence_pool_0.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 16
        }
      }
    }
    persistable: false
  }
  vars {
    name: "lstm_0.w_0@GRAD.trainer_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 16
          dims: 64
        }
      }
    }
  }
  vars {
    name: "__control_var@0.9444312472986823"
    type {
      type: LOD_TENSOR
    }
  }
  vars {
    name: "mean_0.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
        }
      }
    }
    persistable: false
  }
  vars {
    name: "square_error_cost_0.tmp_1@GRAD"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 8
        }
        lod_level: 0
      }
    }
  }
  vars {
    name: "fc_1.tmp_2@GRAD"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 8
        }
        lod_level: 0
      }
    }
  }
  vars {
    name: "lstm_0.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 16
        }
        lod_level: 1
      }
    }
    persistable: false
  }
  vars {
    name: "mean_0.tmp_0@GRAD"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
        }
      }
    }
  }
  vars {
    name: "fc_1.tmp_1@GRAD"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 8
        }
        lod_level: 0
      }
    }
  }
  vars {
    name: "square_error_cost_0.tmp_0@GRAD"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 8
        }
        lod_level: 0
      }
    }
  }
  vars {
    name: "fc_0.w_0@GRAD.trainer_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 64
        }
      }
    }
  }
  vars {
    name: "__control_var@0.22253673750384317"
    type {
      type: LOD_TENSOR
    }
  }
  vars {
    name: "square_error_cost_0.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 8
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "sequence_pool_0.tmp_0@GRAD"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 16
        }
        lod_level: 0
      }
    }
  }
  vars {
    name: "fc_1.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 8
        }
      }
    }
    persistable: true
  }
  vars {
    name: "fc_0.b_0@GRAD.trainer_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
        lod_level: 0
      }
    }
  }
  vars {
    name: "fc_0.tmp_1@GRAD"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 64
        }
      }
    }
  }
  vars {
    name: "fc_1.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 8
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_1.tmp_0@GRAD"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 8
        }
        lod_level: 0
      }
    }
  }
  vars {
    name: "x1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 1
        }
        lod_level: 1
      }
    }
    persistable: false
  }
  vars {
    name: "lstm_0.tmp_3"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 16
        }
      }
    }
    persistable: false
  }
  vars {
    name: "tanh_0.tmp_0@GRAD"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 16
        }
      }
    }
  }
  vars {
    name: "learning_rate_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
        }
      }
    }
    persistable: true
  }
  vars {
    name: "sequence_pool_0.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 16
        }
      }
    }
    persistable: false
  }
  vars {
    name: "fc_0.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 64
        }
        lod_level: 1
      }
    }
    persistable: false
  }
  vars {
    name: "lstm_0.tmp_2"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 64
        }
      }
    }
    persistable: false
  }
  vars {
    name: "__control_var@0.9491989669018565"
    type {
      type: LOD_TENSOR
    }
  }
  vars {
    name: "tanh_0.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 16
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_1.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 8
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_1.w_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 16
          dims: 8
        }
      }
    }
    persistable: true
  }
  vars {
    name: "fc_1.tmp_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 8
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "lstm_0.tmp_0@GRAD"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 16
        }
        lod_level: 1
      }
    }
  }
  vars {
    name: "fc_0.b_0"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 64
        }
      }
    }
    persistable: true
  }
  vars {
    name: "square_error_cost_0.tmp_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: -1
          dims: 8
        }
        lod_level: 0
      }
    }
    persistable: false
  }
  vars {
    name: "fc_1.b_0@GRAD.trainer_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 8
        }
        lod_level: 0
      }
    }
  }
  vars {
    name: "fc_1.w_0@GRAD.trainer_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 16
          dims: 8
        }
      }
    }
  }
  vars {
    name: "lstm_0.b_0@GRAD.trainer_1"
    type {
      type: LOD_TENSOR
      lod_tensor {
        tensor {
          data_type: FP32
          dims: 1
          dims: 112
        }
      }
    }
  }
  vars {
    name: "__control_var@0.6453161088148208"
    type {
      type: LOD_TENSOR
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "x1"
    }
    inputs {
      parameter: "Y"
      arguments: "fc_0.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_0.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "force_fp32_output"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 2459, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 384, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"1.py\", line 17, in lr_network\n    self.fc0 = fluid.layers.fc(input=self.inputs, size=self.hid_dim)\n"
      strings: "  File \"1.py\", line 36, in <module>\n    model.lr_network()\n"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "scale_out"
      type: FLOAT
      f: 1.0
    }
    attrs {
      name: "scale_x"
      type: FLOAT
      f: 1.0
    }
    attrs {
      name: "scale_y"
      type: FLOATS
      floats: 1.0
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_0.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "fc_0.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_0.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 2459, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 397, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"1.py\", line 17, in lr_network\n    self.fc0 = fluid.layers.fc(input=self.inputs, size=self.hid_dim)\n"
      strings: "  File \"1.py\", line 36, in <module>\n    model.lr_network()\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 1
    }
  }
  ops {
    inputs {
      parameter: "Bias"
      arguments: "lstm_0.b_0"
    }
    inputs {
      parameter: "C0"
    }
    inputs {
      parameter: "H0"
    }
    inputs {
      parameter: "Input"
      arguments: "fc_0.tmp_1"
    }
    inputs {
      parameter: "Weight"
      arguments: "lstm_0.w_0"
    }
    outputs {
      parameter: "BatchCellPreAct"
      arguments: "lstm_0.tmp_3"
    }
    outputs {
      parameter: "BatchGate"
      arguments: "lstm_0.tmp_2"
    }
    outputs {
      parameter: "Cell"
      arguments: "lstm_0.tmp_1"
    }
    outputs {
      parameter: "Hidden"
      arguments: "lstm_0.tmp_0"
    }
    type: "lstm"
    attrs {
      name: "candidate_activation"
      type: STRING
      s: "tanh"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "cell_activation"
      type: STRING
      s: "tanh"
    }
    attrs {
      name: "gate_activation"
      type: STRING
      s: "sigmoid"
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 2459, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 843, in dynamic_lstm\n    \'candidate_activation\': candidate_activation\n"
      strings: "  File \"1.py\", line 18, in lr_network\n    self.lstm_h, c = fluid.layers.dynamic_lstm(input=self.fc0, size=self.hid_dim, is_reverse=False)\n"
      strings: "  File \"1.py\", line 36, in <module>\n    model.lr_network()\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "is_reverse"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_peepholes"
      type: BOOLEAN
      b: true
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "lstm_0.tmp_0"
    }
    outputs {
      parameter: "MaxIndex"
      arguments: "sequence_pool_0.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "sequence_pool_0.tmp_0"
    }
    type: "sequence_pool"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 2459, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 3163, in sequence_pool\n    \"pad_value\": pad_value\n"
      strings: "  File \"1.py\", line 21, in lr_network\n    self.lstm_max = fluid.layers.sequence_pool(input=self.lstm_h, pool_type=\'max\')\n"
      strings: "  File \"1.py\", line 36, in <module>\n    model.lr_network()\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "pad_value"
      type: FLOAT
      f: 0.0
    }
    attrs {
      name: "pooltype"
      type: STRING
      s: "MAX"
    }
    attrs {
      name: "is_test"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "sequence_pool_0.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "tanh_0.tmp_0"
    }
    type: "tanh"
    attrs {
      name: "is_test"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_cudnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 2459, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/layers/layer_function_generator.py\", line 267, in func\n    helper.append_op(type=op_type, inputs={\"X\": x}, outputs={\"Out\": output})\n"
      strings: "  File \"1.py\", line 23, in lr_network\n    self.lstm_max_tanh = fluid.layers.tanh(self.lstm_max)\n"
      strings: "  File \"1.py\", line 36, in <module>\n    model.lr_network()\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "tanh_0.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "fc_1.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_1.tmp_0"
    }
    type: "mul"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "force_fp32_output"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 2459, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 384, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"1.py\", line 25, in lr_network\n    self.predict = fluid.layers.fc(input=self.lstm_max_tanh, size=self.hid_dim2, act=\'tanh\')\n"
      strings: "  File \"1.py\", line 36, in <module>\n    model.lr_network()\n"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "scale_out"
      type: FLOAT
      f: 1.0
    }
    attrs {
      name: "scale_x"
      type: FLOAT
      f: 1.0
    }
    attrs {
      name: "scale_y"
      type: FLOATS
      floats: 1.0
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_1.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "fc_1.b_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_1.tmp_1"
    }
    type: "elementwise_add"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 2459, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 397, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"1.py\", line 25, in lr_network\n    self.predict = fluid.layers.fc(input=self.lstm_max_tanh, size=self.hid_dim2, act=\'tanh\')\n"
      strings: "  File \"1.py\", line 36, in <module>\n    model.lr_network()\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
    attrs {
      name: "axis"
      type: INT
      i: 1
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_1.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_1.tmp_2"
    }
    type: "tanh"
    attrs {
      name: "is_test"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_cudnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 2459, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 159, in append_activation\n    attrs=act)\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 399, in fc\n    return helper.append_activation(pre_activation)\n"
      strings: "  File \"1.py\", line 25, in lr_network\n    self.predict = fluid.layers.fc(input=self.lstm_max_tanh, size=self.hid_dim2, act=\'tanh\')\n"
      strings: "  File \"1.py\", line 36, in <module>\n    model.lr_network()\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_1.tmp_2"
    }
    inputs {
      parameter: "Y"
      arguments: "y1"
    }
    outputs {
      parameter: "Out"
      arguments: "square_error_cost_0.tmp_0"
    }
    type: "elementwise_sub"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "axis"
      type: INT
      i: -1
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 2459, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 2041, in square_error_cost\n    outputs={\'Out\': [minus_out]})\n"
      strings: "  File \"1.py\", line 26, in lr_network\n    self.cost = fluid.layers.square_error_cost(input=self.predict, label=self.label)\n"
      strings: "  File \"1.py\", line 36, in <module>\n    model.lr_network()\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "square_error_cost_0.tmp_0"
    }
    outputs {
      parameter: "Out"
      arguments: "square_error_cost_0.tmp_1"
    }
    type: "square"
    attrs {
      name: "is_test"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_cudnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 2459, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 2046, in square_error_cost\n    outputs={\'Out\': [square_out]})\n"
      strings: "  File \"1.py\", line 26, in lr_network\n    self.cost = fluid.layers.square_error_cost(input=self.predict, label=self.label)\n"
      strings: "  File \"1.py\", line 36, in <module>\n    model.lr_network()\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "square_error_cost_0.tmp_1"
    }
    outputs {
      parameter: "Out"
      arguments: "mean_0.tmp_0"
    }
    type: "mean"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 2459, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 14992, in mean\n    type=\"mean\", inputs={\"X\": x}, attrs={}, outputs={\"Out\": out})\n"
      strings: "  File \"1.py\", line 27, in lr_network\n    self.avg_cost = fluid.layers.mean(x=self.cost)\n"
      strings: "  File \"1.py\", line 36, in <module>\n    model.lr_network()\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 256
    }
  }
  ops {
    outputs {
      parameter: "Out"
      arguments: "mean_0.tmp_0@GRAD"
    }
    type: "fill_constant"
    attrs {
      name: "op_role"
      type: INT
      i: 257
    }
    attrs {
      name: "force_cpu"
      type: INT
      i: 0
    }
    attrs {
      name: "dtype"
      type: INT
      i: 5
    }
    attrs {
      name: "value"
      type: FLOAT
      f: 1.0
    }
    attrs {
      name: "shape"
      type: LONGS
      longs: 1
    }
  }
  ops {
    inputs {
      parameter: "Out@GRAD"
      arguments: "mean_0.tmp_0@GRAD"
    }
    inputs {
      parameter: "X"
      arguments: "square_error_cost_0.tmp_1"
    }
    outputs {
      parameter: "X@GRAD"
      arguments: "square_error_cost_0.tmp_1@GRAD"
    }
    type: "mean_grad"
    attrs {
      name: "op_role"
      type: INT
      i: 1
    }
  }
  ops {
    inputs {
      parameter: "Out@GRAD"
      arguments: "square_error_cost_0.tmp_1@GRAD"
    }
    inputs {
      parameter: "X"
      arguments: "square_error_cost_0.tmp_0"
    }
    outputs {
      parameter: "X@GRAD"
      arguments: "square_error_cost_0.tmp_0@GRAD"
    }
    type: "square_grad"
    attrs {
      name: "is_test"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_cudnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 2459, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 2046, in square_error_cost\n    outputs={\'Out\': [square_out]})\n"
      strings: "  File \"1.py\", line 26, in lr_network\n    self.cost = fluid.layers.square_error_cost(input=self.predict, label=self.label)\n"
      strings: "  File \"1.py\", line 36, in <module>\n    model.lr_network()\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 1
    }
  }
  ops {
    inputs {
      parameter: "Out@GRAD"
      arguments: "square_error_cost_0.tmp_0@GRAD"
    }
    inputs {
      parameter: "Y"
      arguments: "y1"
    }
    outputs {
      parameter: "X@GRAD"
      arguments: "fc_1.tmp_2@GRAD"
    }
    outputs {
      parameter: "Y@GRAD"
    }
    type: "elementwise_sub_grad"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "axis"
      type: INT
      i: -1
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 2459, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 2041, in square_error_cost\n    outputs={\'Out\': [minus_out]})\n"
      strings: "  File \"1.py\", line 26, in lr_network\n    self.cost = fluid.layers.square_error_cost(input=self.predict, label=self.label)\n"
      strings: "  File \"1.py\", line 36, in <module>\n    model.lr_network()\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 1
    }
  }
  ops {
    inputs {
      parameter: "Out"
      arguments: "fc_1.tmp_2"
    }
    inputs {
      parameter: "Out@GRAD"
      arguments: "fc_1.tmp_2@GRAD"
    }
    outputs {
      parameter: "X@GRAD"
      arguments: "fc_1.tmp_1@GRAD"
    }
    type: "tanh_grad"
    attrs {
      name: "is_test"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_cudnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 2459, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 159, in append_activation\n    attrs=act)\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 399, in fc\n    return helper.append_activation(pre_activation)\n"
      strings: "  File \"1.py\", line 25, in lr_network\n    self.predict = fluid.layers.fc(input=self.lstm_max_tanh, size=self.hid_dim2, act=\'tanh\')\n"
      strings: "  File \"1.py\", line 36, in <module>\n    model.lr_network()\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 1
    }
  }
  ops {
    inputs {
      parameter: "Out@GRAD"
      arguments: "fc_1.tmp_1@GRAD"
    }
    inputs {
      parameter: "Y"
      arguments: "fc_1.b_0"
    }
    outputs {
      parameter: "X@GRAD"
      arguments: "fc_1.tmp_0@GRAD"
    }
    outputs {
      parameter: "Y@GRAD"
      arguments: "fc_1.b_0@GRAD.trainer_1"
    }
    type: "elementwise_add_grad"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
      strings: "fc_1.b_0"
      strings: "fc_1.b_0@GRAD.trainer_1"
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 2459, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 397, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"1.py\", line 25, in lr_network\n    self.predict = fluid.layers.fc(input=self.lstm_max_tanh, size=self.hid_dim2, act=\'tanh\')\n"
      strings: "  File \"1.py\", line 36, in <module>\n    model.lr_network()\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 1
    }
    attrs {
      name: "axis"
      type: INT
      i: 1
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_1.b_0@GRAD.trainer_1"
    }
    outputs {
      parameter: "Out"
      arguments: "__control_var@0.5934423733003124"
    }
    type: "send"
    attrs {
      name: "use_send_handler"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "epmap"
      type: STRINGS
      strings: "127.0.0.1:8181"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 4
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
      strings: "fc_1.b_0"
      strings: "fc_1.b_0@GRAD.trainer_1"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 2477, in _insert_op\n    op = Operator(block=self, desc=op_desc, *args, **kwargs)\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/transpiler/distribute_transpiler.py\", line 699, in transpile\n    splited_grad_varname\n"
      strings: "  File \"/usr/local/lib/python3.6/dist-packages/paddle_fl-0.1.0-py3.6.egg/paddle_fl/core/strategy/fl_strategy_base.py\", line 163, in _build_trainer_program_for_job\n    startup_program=startup_program)\n"
      strings: "  File \"/usr/local/lib/python3.6/dist-packages/paddle_fl-0.1.0-py3.6.egg/paddle_fl/core/master/job_generator.py\", line 136, in generate_fl_job\n    job=local_job)\n"
      strings: "  File \"1.py\", line 66, in <module>\n    strategy, server_endpoints=endpoints, worker_num=2, output=output)\n"
    }
    attrs {
      name: "merge_add"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "send_varnames"
      type: STRINGS
    }
    attrs {
      name: "sections"
      type: LONGS
    }
    attrs {
      name: "trainer_id"
      type: INT
      i: 0
    }
    attrs {
      name: "num"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "Out@GRAD"
      arguments: "fc_1.tmp_0@GRAD"
    }
    inputs {
      parameter: "X"
      arguments: "tanh_0.tmp_0"
    }
    inputs {
      parameter: "Y"
      arguments: "fc_1.w_0"
    }
    outputs {
      parameter: "X@GRAD"
      arguments: "tanh_0.tmp_0@GRAD"
    }
    outputs {
      parameter: "Y@GRAD"
      arguments: "fc_1.w_0@GRAD.trainer_1"
    }
    type: "mul_grad"
    attrs {
      name: "op_role_var"
      type: STRINGS
      strings: "fc_1.w_0"
      strings: "fc_1.w_0@GRAD.trainer_1"
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "force_fp32_output"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "op_role"
      type: INT
      i: 1
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 2459, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 384, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"1.py\", line 25, in lr_network\n    self.predict = fluid.layers.fc(input=self.lstm_max_tanh, size=self.hid_dim2, act=\'tanh\')\n"
      strings: "  File \"1.py\", line 36, in <module>\n    model.lr_network()\n"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "scale_out"
      type: FLOAT
      f: 1.0
    }
    attrs {
      name: "scale_x"
      type: FLOAT
      f: 1.0
    }
    attrs {
      name: "scale_y"
      type: FLOATS
      floats: 1.0
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_1.w_0@GRAD.trainer_1"
    }
    outputs {
      parameter: "Out"
      arguments: "__control_var@0.9444312472986823"
    }
    type: "send"
    attrs {
      name: "use_send_handler"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "epmap"
      type: STRINGS
      strings: "127.0.0.1:8181"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 4
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
      strings: "fc_1.w_0"
      strings: "fc_1.w_0@GRAD.trainer_1"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 2477, in _insert_op\n    op = Operator(block=self, desc=op_desc, *args, **kwargs)\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/transpiler/distribute_transpiler.py\", line 699, in transpile\n    splited_grad_varname\n"
      strings: "  File \"/usr/local/lib/python3.6/dist-packages/paddle_fl-0.1.0-py3.6.egg/paddle_fl/core/strategy/fl_strategy_base.py\", line 163, in _build_trainer_program_for_job\n    startup_program=startup_program)\n"
      strings: "  File \"/usr/local/lib/python3.6/dist-packages/paddle_fl-0.1.0-py3.6.egg/paddle_fl/core/master/job_generator.py\", line 136, in generate_fl_job\n    job=local_job)\n"
      strings: "  File \"1.py\", line 66, in <module>\n    strategy, server_endpoints=endpoints, worker_num=2, output=output)\n"
    }
    attrs {
      name: "merge_add"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "send_varnames"
      type: STRINGS
    }
    attrs {
      name: "sections"
      type: LONGS
    }
    attrs {
      name: "trainer_id"
      type: INT
      i: 0
    }
    attrs {
      name: "num"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "Out"
      arguments: "tanh_0.tmp_0"
    }
    inputs {
      parameter: "Out@GRAD"
      arguments: "tanh_0.tmp_0@GRAD"
    }
    outputs {
      parameter: "X@GRAD"
      arguments: "sequence_pool_0.tmp_0@GRAD"
    }
    type: "tanh_grad"
    attrs {
      name: "is_test"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_cudnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 2459, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/layers/layer_function_generator.py\", line 267, in func\n    helper.append_op(type=op_type, inputs={\"X\": x}, outputs={\"Out\": output})\n"
      strings: "  File \"1.py\", line 23, in lr_network\n    self.lstm_max_tanh = fluid.layers.tanh(self.lstm_max)\n"
      strings: "  File \"1.py\", line 36, in <module>\n    model.lr_network()\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 1
    }
  }
  ops {
    inputs {
      parameter: "MaxIndex"
      arguments: "sequence_pool_0.tmp_1"
    }
    inputs {
      parameter: "Out@GRAD"
      arguments: "sequence_pool_0.tmp_0@GRAD"
    }
    inputs {
      parameter: "X"
      arguments: "lstm_0.tmp_0"
    }
    outputs {
      parameter: "X@GRAD"
      arguments: "lstm_0.tmp_0@GRAD"
    }
    type: "sequence_pool_grad"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 2459, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 3163, in sequence_pool\n    \"pad_value\": pad_value\n"
      strings: "  File \"1.py\", line 21, in lr_network\n    self.lstm_max = fluid.layers.sequence_pool(input=self.lstm_h, pool_type=\'max\')\n"
      strings: "  File \"1.py\", line 36, in <module>\n    model.lr_network()\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 1
    }
    attrs {
      name: "pad_value"
      type: FLOAT
      f: 0.0
    }
    attrs {
      name: "pooltype"
      type: STRING
      s: "MAX"
    }
    attrs {
      name: "is_test"
      type: BOOLEAN
      b: false
    }
  }
  ops {
    inputs {
      parameter: "BatchCellPreAct"
      arguments: "lstm_0.tmp_3"
    }
    inputs {
      parameter: "BatchGate"
      arguments: "lstm_0.tmp_2"
    }
    inputs {
      parameter: "Bias"
      arguments: "lstm_0.b_0"
    }
    inputs {
      parameter: "C0"
    }
    inputs {
      parameter: "Cell"
      arguments: "lstm_0.tmp_1"
    }
    inputs {
      parameter: "H0"
    }
    inputs {
      parameter: "Hidden"
      arguments: "lstm_0.tmp_0"
    }
    inputs {
      parameter: "Hidden@GRAD"
      arguments: "lstm_0.tmp_0@GRAD"
    }
    inputs {
      parameter: "Input"
      arguments: "fc_0.tmp_1"
    }
    inputs {
      parameter: "Weight"
      arguments: "lstm_0.w_0"
    }
    outputs {
      parameter: "Bias@GRAD"
      arguments: "lstm_0.b_0@GRAD.trainer_1"
    }
    outputs {
      parameter: "C0@GRAD"
    }
    outputs {
      parameter: "H0@GRAD"
    }
    outputs {
      parameter: "Input@GRAD"
      arguments: "fc_0.tmp_1@GRAD"
    }
    outputs {
      parameter: "Weight@GRAD"
      arguments: "lstm_0.w_0@GRAD.trainer_1"
    }
    type: "lstm_grad"
    attrs {
      name: "candidate_activation"
      type: STRING
      s: "tanh"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 1
    }
    attrs {
      name: "cell_activation"
      type: STRING
      s: "tanh"
    }
    attrs {
      name: "gate_activation"
      type: STRING
      s: "sigmoid"
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
      strings: "lstm_0.b_0"
      strings: "lstm_0.b_0@GRAD.trainer_1"
      strings: "lstm_0.w_0"
      strings: "lstm_0.w_0@GRAD.trainer_1"
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 2459, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 843, in dynamic_lstm\n    \'candidate_activation\': candidate_activation\n"
      strings: "  File \"1.py\", line 18, in lr_network\n    self.lstm_h, c = fluid.layers.dynamic_lstm(input=self.fc0, size=self.hid_dim, is_reverse=False)\n"
      strings: "  File \"1.py\", line 36, in <module>\n    model.lr_network()\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "is_reverse"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "use_peepholes"
      type: BOOLEAN
      b: true
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "lstm_0.w_0@GRAD.trainer_1"
    }
    outputs {
      parameter: "Out"
      arguments: "__control_var@0.8252923025577215"
    }
    type: "send"
    attrs {
      name: "use_send_handler"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "epmap"
      type: STRINGS
      strings: "127.0.0.1:8181"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 4
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
      strings: "lstm_0.w_0"
      strings: "lstm_0.w_0@GRAD.trainer_1"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 2477, in _insert_op\n    op = Operator(block=self, desc=op_desc, *args, **kwargs)\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/transpiler/distribute_transpiler.py\", line 699, in transpile\n    splited_grad_varname\n"
      strings: "  File \"/usr/local/lib/python3.6/dist-packages/paddle_fl-0.1.0-py3.6.egg/paddle_fl/core/strategy/fl_strategy_base.py\", line 163, in _build_trainer_program_for_job\n    startup_program=startup_program)\n"
      strings: "  File \"/usr/local/lib/python3.6/dist-packages/paddle_fl-0.1.0-py3.6.egg/paddle_fl/core/master/job_generator.py\", line 136, in generate_fl_job\n    job=local_job)\n"
      strings: "  File \"1.py\", line 66, in <module>\n    strategy, server_endpoints=endpoints, worker_num=2, output=output)\n"
    }
    attrs {
      name: "merge_add"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "send_varnames"
      type: STRINGS
    }
    attrs {
      name: "sections"
      type: LONGS
    }
    attrs {
      name: "trainer_id"
      type: INT
      i: 0
    }
    attrs {
      name: "num"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "lstm_0.b_0@GRAD.trainer_1"
    }
    outputs {
      parameter: "Out"
      arguments: "__control_var@0.22253673750384317"
    }
    type: "send"
    attrs {
      name: "use_send_handler"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "epmap"
      type: STRINGS
      strings: "127.0.0.1:8181"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 4
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
      strings: "lstm_0.b_0"
      strings: "lstm_0.b_0@GRAD.trainer_1"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 2477, in _insert_op\n    op = Operator(block=self, desc=op_desc, *args, **kwargs)\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/transpiler/distribute_transpiler.py\", line 699, in transpile\n    splited_grad_varname\n"
      strings: "  File \"/usr/local/lib/python3.6/dist-packages/paddle_fl-0.1.0-py3.6.egg/paddle_fl/core/strategy/fl_strategy_base.py\", line 163, in _build_trainer_program_for_job\n    startup_program=startup_program)\n"
      strings: "  File \"/usr/local/lib/python3.6/dist-packages/paddle_fl-0.1.0-py3.6.egg/paddle_fl/core/master/job_generator.py\", line 136, in generate_fl_job\n    job=local_job)\n"
      strings: "  File \"1.py\", line 66, in <module>\n    strategy, server_endpoints=endpoints, worker_num=2, output=output)\n"
    }
    attrs {
      name: "merge_add"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "send_varnames"
      type: STRINGS
    }
    attrs {
      name: "sections"
      type: LONGS
    }
    attrs {
      name: "trainer_id"
      type: INT
      i: 0
    }
    attrs {
      name: "num"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "Out@GRAD"
      arguments: "fc_0.tmp_1@GRAD"
    }
    inputs {
      parameter: "Y"
      arguments: "fc_0.b_0"
    }
    outputs {
      parameter: "X@GRAD"
      arguments: "fc_0.tmp_0@GRAD"
    }
    outputs {
      parameter: "Y@GRAD"
      arguments: "fc_0.b_0@GRAD.trainer_1"
    }
    type: "elementwise_add_grad"
    attrs {
      name: "y_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "x_data_format"
      type: STRING
      s: ""
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
      strings: "fc_0.b_0"
      strings: "fc_0.b_0@GRAD.trainer_1"
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 2459, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 135, in append_bias_op\n    attrs={\'axis\': dim_start})\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 397, in fc\n    pre_activation = helper.append_bias_op(pre_bias, dim_start=num_flatten_dims)\n"
      strings: "  File \"1.py\", line 17, in lr_network\n    self.fc0 = fluid.layers.fc(input=self.inputs, size=self.hid_dim)\n"
      strings: "  File \"1.py\", line 36, in <module>\n    model.lr_network()\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 1
    }
    attrs {
      name: "axis"
      type: INT
      i: 1
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_0.b_0@GRAD.trainer_1"
    }
    outputs {
      parameter: "Out"
      arguments: "__control_var@0.9491989669018565"
    }
    type: "send"
    attrs {
      name: "use_send_handler"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "epmap"
      type: STRINGS
      strings: "127.0.0.1:8181"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 4
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
      strings: "fc_0.b_0"
      strings: "fc_0.b_0@GRAD.trainer_1"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 2477, in _insert_op\n    op = Operator(block=self, desc=op_desc, *args, **kwargs)\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/transpiler/distribute_transpiler.py\", line 699, in transpile\n    splited_grad_varname\n"
      strings: "  File \"/usr/local/lib/python3.6/dist-packages/paddle_fl-0.1.0-py3.6.egg/paddle_fl/core/strategy/fl_strategy_base.py\", line 163, in _build_trainer_program_for_job\n    startup_program=startup_program)\n"
      strings: "  File \"/usr/local/lib/python3.6/dist-packages/paddle_fl-0.1.0-py3.6.egg/paddle_fl/core/master/job_generator.py\", line 136, in generate_fl_job\n    job=local_job)\n"
      strings: "  File \"1.py\", line 66, in <module>\n    strategy, server_endpoints=endpoints, worker_num=2, output=output)\n"
    }
    attrs {
      name: "merge_add"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "send_varnames"
      type: STRINGS
    }
    attrs {
      name: "sections"
      type: LONGS
    }
    attrs {
      name: "trainer_id"
      type: INT
      i: 0
    }
    attrs {
      name: "num"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "Out@GRAD"
      arguments: "fc_0.tmp_0@GRAD"
    }
    inputs {
      parameter: "X"
      arguments: "x1"
    }
    inputs {
      parameter: "Y"
      arguments: "fc_0.w_0"
    }
    outputs {
      parameter: "X@GRAD"
    }
    outputs {
      parameter: "Y@GRAD"
      arguments: "fc_0.w_0@GRAD.trainer_1"
    }
    type: "mul_grad"
    attrs {
      name: "op_role_var"
      type: STRINGS
      strings: "fc_0.w_0"
      strings: "fc_0.w_0@GRAD.trainer_1"
    }
    attrs {
      name: "x_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "force_fp32_output"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "y_num_col_dims"
      type: INT
      i: 1
    }
    attrs {
      name: "op_role"
      type: INT
      i: 1
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 2459, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/layer_helper.py\", line 43, in append_op\n    return self.main_program.current_block().append_op(*args, **kwargs)\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/layers/nn.py\", line 384, in fc\n    \"y_num_col_dims\": 1})\n"
      strings: "  File \"1.py\", line 17, in lr_network\n    self.fc0 = fluid.layers.fc(input=self.inputs, size=self.hid_dim)\n"
      strings: "  File \"1.py\", line 36, in <module>\n    model.lr_network()\n"
    }
    attrs {
      name: "use_mkldnn"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "scale_out"
      type: FLOAT
      f: 1.0
    }
    attrs {
      name: "scale_x"
      type: FLOAT
      f: 1.0
    }
    attrs {
      name: "scale_y"
      type: FLOATS
      floats: 1.0
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "fc_0.w_0@GRAD.trainer_1"
    }
    outputs {
      parameter: "Out"
      arguments: "__control_var@0.6453161088148208"
    }
    type: "send"
    attrs {
      name: "use_send_handler"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "epmap"
      type: STRINGS
      strings: "127.0.0.1:8181"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 4
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
      strings: "fc_0.w_0"
      strings: "fc_0.w_0@GRAD.trainer_1"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 2477, in _insert_op\n    op = Operator(block=self, desc=op_desc, *args, **kwargs)\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/transpiler/distribute_transpiler.py\", line 699, in transpile\n    splited_grad_varname\n"
      strings: "  File \"/usr/local/lib/python3.6/dist-packages/paddle_fl-0.1.0-py3.6.egg/paddle_fl/core/strategy/fl_strategy_base.py\", line 163, in _build_trainer_program_for_job\n    startup_program=startup_program)\n"
      strings: "  File \"/usr/local/lib/python3.6/dist-packages/paddle_fl-0.1.0-py3.6.egg/paddle_fl/core/master/job_generator.py\", line 136, in generate_fl_job\n    job=local_job)\n"
      strings: "  File \"1.py\", line 66, in <module>\n    strategy, server_endpoints=endpoints, worker_num=2, output=output)\n"
    }
    attrs {
      name: "merge_add"
      type: BOOLEAN
      b: false
    }
    attrs {
      name: "send_varnames"
      type: STRINGS
    }
    attrs {
      name: "sections"
      type: LONGS
    }
    attrs {
      name: "trainer_id"
      type: INT
      i: 0
    }
    attrs {
      name: "num"
      type: INT
      i: 0
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "__control_var@0.9491989669018565"
      arguments: "__control_var@0.6453161088148208"
      arguments: "__control_var@0.5934423733003124"
      arguments: "__control_var@0.9444312472986823"
      arguments: "__control_var@0.22253673750384317"
      arguments: "__control_var@0.8252923025577215"
    }
    outputs {
      parameter: "Out"
      arguments: "__control_var@0.6105734521044512"
    }
    type: "send_barrier"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 2459, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/transpiler/distribute_transpiler.py\", line 722, in transpile\n    RPC_OP_ROLE_ATTR_NAME: RPC_OP_ROLE_ATTR_VALUE\n"
      strings: "  File \"/usr/local/lib/python3.6/dist-packages/paddle_fl-0.1.0-py3.6.egg/paddle_fl/core/strategy/fl_strategy_base.py\", line 163, in _build_trainer_program_for_job\n    startup_program=startup_program)\n"
      strings: "  File \"/usr/local/lib/python3.6/dist-packages/paddle_fl-0.1.0-py3.6.egg/paddle_fl/core/master/job_generator.py\", line 136, in generate_fl_job\n    job=local_job)\n"
      strings: "  File \"1.py\", line 66, in <module>\n    strategy, server_endpoints=endpoints, worker_num=2, output=output)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 4
    }
    attrs {
      name: "endpoints"
      type: STRINGS
      strings: "127.0.0.1:8181"
    }
    attrs {
      name: "trainer_id"
      type: INT
      i: 1
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "__control_var@0.6105734521044512"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_0.b_0"
    }
    type: "recv"
    attrs {
      name: "varnames"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 2459, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/transpiler/distribute_transpiler.py\", line 818, in transpile\n    [param_varname, recv_op_role_var_name]\n"
      strings: "  File \"/usr/local/lib/python3.6/dist-packages/paddle_fl-0.1.0-py3.6.egg/paddle_fl/core/strategy/fl_strategy_base.py\", line 163, in _build_trainer_program_for_job\n    startup_program=startup_program)\n"
      strings: "  File \"/usr/local/lib/python3.6/dist-packages/paddle_fl-0.1.0-py3.6.egg/paddle_fl/core/master/job_generator.py\", line 136, in generate_fl_job\n    job=local_job)\n"
      strings: "  File \"1.py\", line 66, in <module>\n    strategy, server_endpoints=endpoints, worker_num=2, output=output)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
      strings: "fc_0.b_0"
      strings: "fc_0.b_0@GRAD.trainer_1"
    }
    attrs {
      name: "with_barrier"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "recv_varnames"
      type: STRINGS
    }
    attrs {
      name: "do_not_run"
      type: INT
      i: 0
    }
    attrs {
      name: "trainer_id"
      type: INT
      i: 1
    }
    attrs {
      name: "op_role"
      type: INT
      i: 4
    }
    attrs {
      name: "epmap"
      type: STRINGS
      strings: "127.0.0.1:8181"
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "__control_var@0.6105734521044512"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_0.w_0"
    }
    type: "recv"
    attrs {
      name: "varnames"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 2459, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/transpiler/distribute_transpiler.py\", line 818, in transpile\n    [param_varname, recv_op_role_var_name]\n"
      strings: "  File \"/usr/local/lib/python3.6/dist-packages/paddle_fl-0.1.0-py3.6.egg/paddle_fl/core/strategy/fl_strategy_base.py\", line 163, in _build_trainer_program_for_job\n    startup_program=startup_program)\n"
      strings: "  File \"/usr/local/lib/python3.6/dist-packages/paddle_fl-0.1.0-py3.6.egg/paddle_fl/core/master/job_generator.py\", line 136, in generate_fl_job\n    job=local_job)\n"
      strings: "  File \"1.py\", line 66, in <module>\n    strategy, server_endpoints=endpoints, worker_num=2, output=output)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
      strings: "fc_0.w_0"
      strings: "fc_0.w_0@GRAD.trainer_1"
    }
    attrs {
      name: "with_barrier"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "recv_varnames"
      type: STRINGS
    }
    attrs {
      name: "do_not_run"
      type: INT
      i: 0
    }
    attrs {
      name: "trainer_id"
      type: INT
      i: 1
    }
    attrs {
      name: "op_role"
      type: INT
      i: 4
    }
    attrs {
      name: "epmap"
      type: STRINGS
      strings: "127.0.0.1:8181"
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "__control_var@0.6105734521044512"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_1.b_0"
    }
    type: "recv"
    attrs {
      name: "varnames"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 2459, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/transpiler/distribute_transpiler.py\", line 818, in transpile\n    [param_varname, recv_op_role_var_name]\n"
      strings: "  File \"/usr/local/lib/python3.6/dist-packages/paddle_fl-0.1.0-py3.6.egg/paddle_fl/core/strategy/fl_strategy_base.py\", line 163, in _build_trainer_program_for_job\n    startup_program=startup_program)\n"
      strings: "  File \"/usr/local/lib/python3.6/dist-packages/paddle_fl-0.1.0-py3.6.egg/paddle_fl/core/master/job_generator.py\", line 136, in generate_fl_job\n    job=local_job)\n"
      strings: "  File \"1.py\", line 66, in <module>\n    strategy, server_endpoints=endpoints, worker_num=2, output=output)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
      strings: "fc_1.b_0"
      strings: "fc_1.b_0@GRAD.trainer_1"
    }
    attrs {
      name: "with_barrier"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "recv_varnames"
      type: STRINGS
    }
    attrs {
      name: "do_not_run"
      type: INT
      i: 0
    }
    attrs {
      name: "trainer_id"
      type: INT
      i: 1
    }
    attrs {
      name: "op_role"
      type: INT
      i: 4
    }
    attrs {
      name: "epmap"
      type: STRINGS
      strings: "127.0.0.1:8181"
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "__control_var@0.6105734521044512"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_1.w_0"
    }
    type: "recv"
    attrs {
      name: "varnames"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 2459, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/transpiler/distribute_transpiler.py\", line 818, in transpile\n    [param_varname, recv_op_role_var_name]\n"
      strings: "  File \"/usr/local/lib/python3.6/dist-packages/paddle_fl-0.1.0-py3.6.egg/paddle_fl/core/strategy/fl_strategy_base.py\", line 163, in _build_trainer_program_for_job\n    startup_program=startup_program)\n"
      strings: "  File \"/usr/local/lib/python3.6/dist-packages/paddle_fl-0.1.0-py3.6.egg/paddle_fl/core/master/job_generator.py\", line 136, in generate_fl_job\n    job=local_job)\n"
      strings: "  File \"1.py\", line 66, in <module>\n    strategy, server_endpoints=endpoints, worker_num=2, output=output)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
      strings: "fc_1.w_0"
      strings: "fc_1.w_0@GRAD.trainer_1"
    }
    attrs {
      name: "with_barrier"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "recv_varnames"
      type: STRINGS
    }
    attrs {
      name: "do_not_run"
      type: INT
      i: 0
    }
    attrs {
      name: "trainer_id"
      type: INT
      i: 1
    }
    attrs {
      name: "op_role"
      type: INT
      i: 4
    }
    attrs {
      name: "epmap"
      type: STRINGS
      strings: "127.0.0.1:8181"
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "__control_var@0.6105734521044512"
    }
    outputs {
      parameter: "Out"
      arguments: "lstm_0.b_0"
    }
    type: "recv"
    attrs {
      name: "varnames"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 2459, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/transpiler/distribute_transpiler.py\", line 818, in transpile\n    [param_varname, recv_op_role_var_name]\n"
      strings: "  File \"/usr/local/lib/python3.6/dist-packages/paddle_fl-0.1.0-py3.6.egg/paddle_fl/core/strategy/fl_strategy_base.py\", line 163, in _build_trainer_program_for_job\n    startup_program=startup_program)\n"
      strings: "  File \"/usr/local/lib/python3.6/dist-packages/paddle_fl-0.1.0-py3.6.egg/paddle_fl/core/master/job_generator.py\", line 136, in generate_fl_job\n    job=local_job)\n"
      strings: "  File \"1.py\", line 66, in <module>\n    strategy, server_endpoints=endpoints, worker_num=2, output=output)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
      strings: "lstm_0.b_0"
      strings: "lstm_0.b_0@GRAD.trainer_1"
    }
    attrs {
      name: "with_barrier"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "recv_varnames"
      type: STRINGS
    }
    attrs {
      name: "do_not_run"
      type: INT
      i: 0
    }
    attrs {
      name: "trainer_id"
      type: INT
      i: 1
    }
    attrs {
      name: "op_role"
      type: INT
      i: 4
    }
    attrs {
      name: "epmap"
      type: STRINGS
      strings: "127.0.0.1:8181"
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "__control_var@0.6105734521044512"
    }
    outputs {
      parameter: "Out"
      arguments: "lstm_0.w_0"
    }
    type: "recv"
    attrs {
      name: "varnames"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 2459, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/transpiler/distribute_transpiler.py\", line 818, in transpile\n    [param_varname, recv_op_role_var_name]\n"
      strings: "  File \"/usr/local/lib/python3.6/dist-packages/paddle_fl-0.1.0-py3.6.egg/paddle_fl/core/strategy/fl_strategy_base.py\", line 163, in _build_trainer_program_for_job\n    startup_program=startup_program)\n"
      strings: "  File \"/usr/local/lib/python3.6/dist-packages/paddle_fl-0.1.0-py3.6.egg/paddle_fl/core/master/job_generator.py\", line 136, in generate_fl_job\n    job=local_job)\n"
      strings: "  File \"1.py\", line 66, in <module>\n    strategy, server_endpoints=endpoints, worker_num=2, output=output)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role_var"
      type: STRINGS
      strings: "lstm_0.w_0"
      strings: "lstm_0.w_0@GRAD.trainer_1"
    }
    attrs {
      name: "with_barrier"
      type: BOOLEAN
      b: true
    }
    attrs {
      name: "recv_varnames"
      type: STRINGS
    }
    attrs {
      name: "do_not_run"
      type: INT
      i: 0
    }
    attrs {
      name: "trainer_id"
      type: INT
      i: 1
    }
    attrs {
      name: "op_role"
      type: INT
      i: 4
    }
    attrs {
      name: "epmap"
      type: STRINGS
      strings: "127.0.0.1:8181"
    }
  }
  ops {
    inputs {
      parameter: "X"
      arguments: "__control_var@0.6105734521044512"
      arguments: "fc_0.b_0"
      arguments: "fc_0.w_0"
      arguments: "fc_1.b_0"
      arguments: "fc_1.w_0"
      arguments: "lstm_0.b_0"
      arguments: "lstm_0.w_0"
    }
    outputs {
      parameter: "Out"
      arguments: "fc_0.b_0"
      arguments: "fc_0.w_0"
      arguments: "fc_1.b_0"
      arguments: "fc_1.w_0"
      arguments: "lstm_0.b_0"
      arguments: "lstm_0.w_0"
    }
    type: "fetch_barrier"
    attrs {
      name: "op_role_var"
      type: STRINGS
    }
    attrs {
      name: "op_callstack"
      type: STRINGS
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/framework.py\", line 2459, in append_op\n    attrs=kwargs.get(\"attrs\", None))\n"
      strings: "  File \"/home/hoy/.local/lib/python3.6/site-packages/paddle/fluid/transpiler/distribute_transpiler.py\", line 834, in transpile\n    RPC_OP_ROLE_ATTR_NAME: RPC_OP_ROLE_ATTR_VALUE\n"
      strings: "  File \"/usr/local/lib/python3.6/dist-packages/paddle_fl-0.1.0-py3.6.egg/paddle_fl/core/strategy/fl_strategy_base.py\", line 163, in _build_trainer_program_for_job\n    startup_program=startup_program)\n"
      strings: "  File \"/usr/local/lib/python3.6/dist-packages/paddle_fl-0.1.0-py3.6.egg/paddle_fl/core/master/job_generator.py\", line 136, in generate_fl_job\n    job=local_job)\n"
      strings: "  File \"1.py\", line 66, in <module>\n    strategy, server_endpoints=endpoints, worker_num=2, output=output)\n"
    }
    attrs {
      name: "op_namescope"
      type: STRING
      s: "/"
    }
    attrs {
      name: "op_role"
      type: INT
      i: 4
    }
    attrs {
      name: "endpoints"
      type: STRINGS
      strings: "127.0.0.1:8181"
    }
    attrs {
      name: "trainer_id"
      type: INT
      i: 1
    }
  }
}
version {
  version: 1006001
}
